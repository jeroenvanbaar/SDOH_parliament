{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08563a49",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib, os,sys, pdfplumber, glob, requests, wordcloud, re, dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192c128",
   "metadata": {},
   "source": [
    "# Set up working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca98c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.realpath('../..')\n",
    "print(base_dir)\n",
    "data_dir = base_dir + '\\Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288ea3e",
   "metadata": {},
   "source": [
    "# Load text files with date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = data_dir + '\\\\TK_commissieVWS\\\\auto_download_20230118'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159eb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(in_dir + '\\\\text\\\\*.txt')\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f091db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.DataFrame(columns = ['file_nr','doc_nr','doc_id','date','name','text','nr_of_words'])\n",
    "for fi,fpath in enumerate(all_files):\n",
    "    if np.mod(fi,10) == 0:\n",
    "        print(fi, end = ', ')\n",
    "    fname = fpath.split('\\\\')[-1]\n",
    "    [doc_nr, doc_id, doc_date, doc_name1, doc_name2] = fname.split('_')\n",
    "    doc_nr = int(doc_nr[3:])\n",
    "    doc_date = dateparser.parse(doc_date)\n",
    "    doc_name = doc_name1 + doc_name2\n",
    "    try:\n",
    "        doc_text = open(fpath, \"r\", encoding=\"utf8\").read().lower() # Important: removed the wiping of newlines here\n",
    "        doc_nr_of_words = len(re.findall(r'\\w+', doc_text))\n",
    "        all_text = all_text.append(pd.DataFrame([[fi, doc_nr, doc_id, doc_date,doc_name,doc_text,doc_nr_of_words]],\n",
    "                                            columns = all_text.columns))\n",
    "    except:\n",
    "        print('Skipping %s'%fname)\n",
    "all_text = all_text.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example\n",
    "all_text.loc[all_text.date == '2021-09-15','text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1299ca57",
   "metadata": {},
   "source": [
    "## Add missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_xtol(fpath):\n",
    "    \n",
    "    # Find\n",
    "    print('>> %s'%fpath.split('\\\\')[-1])\n",
    "\n",
    "    # Read\n",
    "    pdf = pdfplumber.open(fpath)\n",
    "    cover_page = pdf.pages[0]\n",
    "    content_pages = pdf.pages[2:]\n",
    "    \n",
    "    ### 2. Determine number of columns in page somewhere in the middle\n",
    "    if len(content_pages) > 5:\n",
    "        page = content_pages[int(len(content_pages)/2+2)] # This is to ensure that we don't evaluate the first page but also don't go over\n",
    "    elif len(content_pages) > 1:\n",
    "        page = content_pages[1]\n",
    "    else:\n",
    "        page = content_pages[0]\n",
    "\n",
    "    n_slivers = 100\n",
    "    sliver_edges = np.round(np.linspace(0,1,n_slivers+1), decimals = 3)\n",
    "    sliver_contents = []\n",
    "    for sliver_start, sliver_end in zip(sliver_edges[:-1],sliver_edges[1:]):\n",
    "        bbox = (sliver_start*float(page.width), 0, sliver_end*float(page.width), float(page.height))\n",
    "        sliver = page.crop(bbox)\n",
    "        sliver_text = sliver.extract_text()\n",
    "        sliver_text = sliver_text.replace('\\n','') if sliver_text is not None else ''\n",
    "        sliver_contents.append(len(sliver_text))\n",
    "\n",
    "    sliver_yesno = sliver_contents > np.mean(sliver_contents)\n",
    "    n_cols = int(sum(np.diff(sliver_yesno)) / 2)\n",
    "    n_cols = 2 if (n_cols == 2 and sliver_yesno[50] == False) else 1 # Double check: the middle sliver must be below the mean text density (i.e. whitespace in the middle between 2 columns)\n",
    "    print('Number of columns: %i'%n_cols)\n",
    "\n",
    "    ### 3. Extract text with appropriate x tolerance\n",
    "\n",
    "    # Extract text\n",
    "    acceptable_word_length = False\n",
    "    xtol = 3\n",
    "    left_bbox = (0, 0, 0.5*float(page.width), float(page.height))\n",
    "    right_bbox = (0.5*float(page.width), 0, float(page.width), float(page.height))\n",
    "\n",
    "    while acceptable_word_length == False:\n",
    "        verslag_pages = []\n",
    "\n",
    "        # Loop over content pages\n",
    "        print('Reading pdf...')\n",
    "        for page in content_pages:\n",
    "\n",
    "            # Read by column\n",
    "            if n_cols == 2:\n",
    "                left_col = page.crop(left_bbox)\n",
    "                left_col_text = left_col.extract_text(x_tolerance = xtol)\n",
    "                if left_col_text is not None:\n",
    "                    right_col = page.crop(right_bbox)\n",
    "                    right_col_text = right_col.extract_text(x_tolerance = xtol)\n",
    "                    page_text = left_col_text + ' ' + right_col_text\n",
    "                else:\n",
    "                    page_text = left_col_text # If left column is empty, we skip the right column\n",
    "            elif n_cols == 1:\n",
    "                page_text = page.extract_text(x_tolerance = xtol)\n",
    "            else:\n",
    "                raise ValueError('Impossible number of columns: %i'%n_cols)\n",
    "\n",
    "            # Check for empty pages\n",
    "            if page_text == None:\n",
    "                print('Skipping %s'%page)\n",
    "            else:\n",
    "                verslag_pages.append(page_text)\n",
    "        verslag_text = ' '.join(verslag_pages)\n",
    "\n",
    "        # Check mean word length and repeat if necessary\n",
    "        mean_word_length = np.divide(len(verslag_text), len(verslag_text.split(' ')))\n",
    "        if mean_word_length > 40:\n",
    "            if xtol == 1: # If the xtol is already at 1, skip this file\n",
    "                print('Mean word length = %.2f, still too long. Skipping doc...'%mean_word_length)\n",
    "                return\n",
    "            else: # Otherwise lower the xtol to 1\n",
    "                print('Mean word length = %.2f, lowering space x tolerance to 1.'%mean_word_length)\n",
    "                xtol = 1\n",
    "        else:\n",
    "            print('Continuing with mean word length = %.2f.'%mean_word_length)\n",
    "            acceptable_word_length = True\n",
    "    \n",
    "    return xtol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9039249",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_find_date = all_text.loc[pd.isnull(all_text['date']),'doc_id'].tolist()\n",
    "[print(a, end = ', ') for a in ids_to_find_date];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca76591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for doc_id in ids_to_find_date:\n",
    "    print('Doc ID = %s'%doc_id)\n",
    "    matches = glob.glob(in_dir + '\\\\original\\\\*%s*.pdf'%doc_id)\n",
    "    if len(matches)>1:\n",
    "        raise ValueError('Too many matches')\n",
    "    else:\n",
    "        fpath = matches[0]\n",
    "        xtol = find_xtol(fpath)\n",
    "        pdf = pdfplumber.open(fpath)\n",
    "        cover_page = pdf.pages[0]\n",
    "        cover_page_text = cover_page.extract_text(x_tolerance = xtol).replace('\\n', ' ')\n",
    "        after_vastgesteld = cover_page_text[cover_page_text.find('astgesteld')+30:]\n",
    "        str_to_find = '\\s+\\d\\d?\\s+\\w+\\s+\\d\\d\\d\\d'\n",
    "        # 'op\\s\\d+\\s\\w+\\s\\d+\\soverleg' # Sometimes it's \"heeft op donderdag 25 mei 2016 overleg...\" or \"hebben op 4 juni gesprekken gevoerd...\" so the text is not consistent enough to match anything else than the date\n",
    "        date_found = re.findall(str_to_find, after_vastgesteld)[0]\n",
    "        date_formatted = dateparser.parse(date_found)\n",
    "        date_string = date_formatted.strftime('%Y.%m.%d')\n",
    "        print('Datum vergadering: %s'%date_string)\n",
    "        \n",
    "        all_text.loc[all_text['doc_id']==doc_id,'date'] = date_formatted\n",
    "        print('Date saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text.to_csv(in_dir + '\\\\text\\\\all_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552803c1",
   "metadata": {},
   "source": [
    "## Split by speaking turn, label party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.read_csv(in_dir + '\\\\text\\\\all_text.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00dc51",
   "metadata": {},
   "source": [
    "This function splits text into speaking turns using the format of the verslagen: e.g. \"meneer Van Haga (FVD):\". It also replaces newline characters by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd33d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_speaking_turns_explicit(in_text, verbose = False):\n",
    "    # Split the text by all possible speakers: de heer, mevrouw, voorzitter, minister.\n",
    "    in_text = in_text.replace('tweede kamer, vergaderjaar','')\n",
    "    turns_list = re.split(r'(de voorzitter:)|(mevrouw.*?\\):)|(de heer.*?\\):)|(minister.*?\\:)', in_text)\n",
    "    turns_list = [t for t in turns_list if t is not None]\n",
    "    turns_df = pd.DataFrame()\n",
    "    turns_df['speaker'] = turns_list[1::2]\n",
    "    turns_df['text'] = turns_list[2::2]\n",
    "    turns_df_clean = pd.DataFrame()\n",
    "    for ti,turn in turns_df.iterrows():\n",
    "        turn_text = turn['text'].replace('-\\n','')\n",
    "        turn_text = turn_text.replace('\\n',' ')\n",
    "        if verbose:\n",
    "            print(ti,end = ',')\n",
    "        if 'voorzitter' in turn['speaker']:\n",
    "            name_clean = 'voorzitter'\n",
    "            gender = 'n/a'\n",
    "            party = 'n/a'\n",
    "        elif 'minister' in turn['speaker']:\n",
    "            name_clean = 'minister'\n",
    "            gender = 'n/a'\n",
    "            party = 'n/a'\n",
    "        else:\n",
    "            name = turn['speaker'].split('(')[0]\n",
    "            party = turn['speaker'].split('(')[1].split(')')[0]\n",
    "            if 'mevrouw' in turn['speaker']:\n",
    "                gender = 'mevrouw'\n",
    "                name_clean = name.split('mevrouw')[1].strip(' ')\n",
    "            else:\n",
    "                gender = 'de heer'\n",
    "                name_clean = name.split('de heer')[1].strip(' ')\n",
    "        tmp = pd.DataFrame([[ti,gender,name_clean,party,turn_text]],\n",
    "                           columns = ['turn_nr','gender','name','party','text'])\n",
    "        turns_df_clean = turns_df_clean.append(tmp).reset_index(drop=True)\n",
    "    if verbose:\n",
    "        print('\\n\\n%i speaking turns found for parties %s'%(turns_df_clean.shape[0], sorted(turns_df_clean['party'].unique())))\n",
    "    return turns_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7bf577",
   "metadata": {},
   "source": [
    "##### Loop and parse all verslagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "verslagen_in = all_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speaking_turns = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ec264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for di,doc_info in verslagen_in.iterrows():\n",
    "    speaking_turns = split_speaking_turns_explicit(doc_info['text'])\n",
    "    speaking_turns['date'] = doc_info['date']\n",
    "    speaking_turns['doc_nr'] = doc_info['doc_nr']\n",
    "    speaking_turns['doc_id'] = doc_info['doc_id']\n",
    "    n_speaking_turns = int(np.shape(speaking_turns)[0])\n",
    "    all_speaking_turns = all_speaking_turns.append(speaking_turns)\n",
    "    print(di,'Doc %i done, date %s, %i speaking turns found'%(\n",
    "        doc_info['doc_nr'],doc_info['date'],n_speaking_turns))\n",
    "    verslagen_in.loc[di,'nr_of_turns'] = n_speaking_turns\n",
    "all_speaking_turns = all_speaking_turns.reset_index(drop=True)\n",
    "all_speaking_turns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speaking_turns['party'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speaking_turns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f26c4",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speaking_turns.to_csv(in_dir + '\\speaking_turns.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
